\section{Glossary}

\begin{itemize}
    \item \textbf{KG4Vis} - Knowledge Graph for Visualization Recommendation.
    \item \textbf{VizML} - Visualization using Machine Learning.
    \item \textbf{CUDA} - Compute Unified Device Architecture.
    \item \textbf{TSV} - Tab-Separated Values.
    \item \textbf{CSV} - Comma-Separated Values.
    \item \textbf{GPU} - Graphics Processing Unit.
    \item \textbf{CPU} - Central Processing Unit.
    \item \textbf{RL} - Reinforcement Learning.
\end{itemize}
\vspace{18.0cm}

\section{Concepts}

\vspace{0.3cm}
\noindent\textbf{Activation function:} A function used in neural networks to introduce nonlinearity into the model. Common examples include ReLU, Sigmoid, and Softmax.

\vspace{0.3cm}
\noindent\textbf{Adam optimizer:} An adaptive learning rate optimization algorithm that combines the advantages of AdaGrad and RMSProp. Widely used for training deep learning models.

\vspace{0.3cm}
\noindent\textbf{AutoGrad:} PyTorch’s automatic differentiation engine, which computes gradients for optimization during backpropagation.

\vspace{0.3cm}
\noindent\textbf{Batch size:} The number of training samples processed in one forward/backward pass during model training. Larger batches offer more stable gradient estimates, but consume more memory.

\vspace{0.3cm}
\noindent\textbf{DataLoader:} A PyTorch utility that provides an efficient way to load data in mini-batches and shuffle them during training.

\vspace{0.3cm}
\noindent\textbf{Distributed computing:} A computing model where multiple computers (or nodes) work together over a network to solve a large problem.

\vspace{0.3cm}
\noindent\textbf{Embedding:} A representation of complex data as a vector of numbers in a continuous space, so that similar things have similar vectors.

\vspace{0.3cm}
\noindent\textbf{Feedforward neural network:} A type of neural network in which information moves in only one direction, from input to output, without cycles or loops.

\vspace{0.3cm}
\noindent\textbf{Graph partitioning:} The process of dividing a graph into smaller parts that have roughly the same number of nodes, with the objective of minimizing the number of edges between parts.

\vspace{0.3cm}
\noindent\textbf{Knowledge graph:} A structured way to represent information using a graph of real-world entities and their relationships, where the nodes represent the entities and the edges represent the relationships.

\vspace{0.3cm}
\noindent\textbf{Learning rate scheduling:} A technique for dynamically adjusting the learning rate during training, often to improve convergence and performance.

\vspace{0.3cm}
\noindent\textbf{Loss function:} A function that measures how far off the network’s predictions are from the actual labels. Common examples include cross-entropy and mean squared error.

\vspace{0.3cm}
\noindent\textbf{Mean Rank (MR):} A ranking-based evaluation metric that represents the average rank position of the correct visualization type in a list of recommendations. It measures how well the model ranks correct options—performance is better when the MR is lower.

\vspace{0.3cm}
\noindent\textbf{Mini-batch training:} A training approach where a subset (batch) of the dataset is used to update the model weights rather than using the entire dataset.

\vspace{0.3cm}
\noindent\textbf{Negative sampling:} A training technique used in knowledge graph embeddings to generate incorrect (negative) examples, helping the model distinguish valid from invalid relationships.

\vspace{0.3cm}
\noindent\textbf{Neural network:} A machine learning model that recognizes patterns and makes predictions by adjusting connections between layers of artificial "neurons."

\vspace{0.3cm}
\noindent\textbf{Neural network topology:} The way that neurons are connected in a neural network, defining the number of layers and neurons, how they are connected, and what types of layers are used.

\vspace{0.3cm}
\noindent\textbf{PyTorch:} An open-source machine learning framework based on Python and Torch that is widely used for developing deep learning applications.

\vspace{0.3cm}
\noindent\textbf{ReLU (Rectified Linear Unit):} A popular activation function defined as \( f(x) = \max(0, x) \), used to introduce non-linearity into neural networks while avoiding the vanishing gradient problem.

\vspace{0.3cm}
\noindent\textbf{Simple classifiers:} Less complex machine learning models that assign labels to data using basic decision rules or statistical techniques.

\vspace{0.3cm}
\noindent\textbf{Softmax:} An activation function that converts a vector of values into a probability distribution. Often used in the output layer of classification tasks.

\vspace{0.3cm}
\noindent\textbf{Torch variables:} Multi-dimensional arrays used in PyTorch that support operations like matrix multiplication, addition, reshaping, etc. They can reside in CPU or GPU memory for efficient computation.

\vspace{0.3cm}
\noindent\textbf{TransE embedding:} A knowledge graph embedding model that represents entities and relationships as vectors in a continuous space. It allows KG4Vis to infer new relationships and make explainable recommendations.
